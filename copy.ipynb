{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "copy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HONO1rKBaG9q"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg \n",
        "from tqdm import tqdm # Displays a progress bar\n",
        "from PIL import Image as PImage\n",
        "from pathlib import Path\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import shutil\n",
        "import math\n",
        "import h5py\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F   \n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, random_split, ConcatDataset\n",
        "import torchvision.models as models\n",
        "\n",
        "%cd /content\n",
        "!gdown https://drive.google.com/uc?id=1Lqk7z7iVPG7vRl5-wcY0GPjjYwAy7Kfy\n",
        "#https://drive.google.com/file/d/1Lqk7z7iVPG7vRl5-wcY0GPjjYwAy7Kfy/view\n",
        "!unzip /content/DATA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iKESfYQa375"
      },
      "source": [
        "params = {\n",
        "    \"batchSize\": 32,\n",
        "    \"epochs\": 16,\n",
        "    \"learningRate\": 0.001,\n",
        "    \"frozenChildren\": 9,\n",
        "\n",
        "}\n",
        "folders = ['Covid', 'Healthy', 'Others']\n",
        "newfolders = []\n",
        "\n",
        "image_label = []\n",
        "\n",
        "max_length = []\n",
        "\n",
        "averageAccuracy1 = []\n",
        "averageAccuracy2 = []\n",
        "  \n",
        "%cd /content/496data/\n",
        "origDir = os.getcwd()\n",
        "allDir = os.path.join(origDir, 'AllImages')\n",
        "if os.path.isdir(allDir):\n",
        "    shutil.rmtree(allDir)\n",
        "os.mkdir(allDir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rVOJUTRbfzT",
        "outputId": "d23c6a13-239b-4930-9ade-334841ec0d1d"
      },
      "source": [
        "def combine_images(folderType):\n",
        "    thisDir = os.path.join(origDir, folderType)\n",
        "    newDir = os.path.join(origDir, 'new'+folderType)\n",
        "    \n",
        "    if os.path.isdir(newDir):\n",
        "        shutil.rmtree(newDir)\n",
        "    os.mkdir(newDir)\n",
        "    newfolders.append(newDir)\n",
        "    \n",
        "    count = 0\n",
        "    \n",
        "    for patient in os.listdir(thisDir):\n",
        "        patientPath = os.path.join(thisDir, patient)\n",
        "        if os.path.isdir(patientPath):\n",
        "            for file in glob.glob(os.path.join(patientPath, '*.png')):\n",
        "                count+=1\n",
        "                imagePath = os.path.join(patientPath, file)\n",
        "                img = cv2.imread(imagePath)\n",
        "                img = cv2.resize(img,  (224,224), interpolation=cv2.INTER_CUBIC)\n",
        "                newPath = os.path.join(newDir, str(count) + '.png')\n",
        "                cv2.imwrite(newPath, img)\n",
        "                # shutil.copy(imagePath, newPath)\n",
        "                \n",
        "    \n",
        "for folder in folders:\n",
        "    combine_images(folder)\n",
        "    \n",
        "    \n",
        "def combine_images2(folderType):\n",
        "    count = 0\n",
        "    label = -1\n",
        "    \n",
        "    for folder in folderType:\n",
        "        label+=1\n",
        "        for image in os.listdir(folder):\n",
        "            count+=1\n",
        "            max_length.append(count)\n",
        "            imagePath = os.path.join(folder, image)\n",
        "            newPath = os.path.join(allDir, str(count) + '.png')\n",
        "            shutil.copy(imagePath, newPath)\n",
        "            img = cv2.imread(newPath)\n",
        "            image_label.append([newPath, label, count])\n",
        "            img = cv2.flip(img, 1)\n",
        "            count+=1\n",
        "            max_length.append(count)\n",
        "            newPath3 = os.path.join(allDir, str(count) + '.png')\n",
        "            cv2.imwrite(newPath3, img)\n",
        "            image_label.append([newPath3, label, count])\n",
        "            for i in range(3):\n",
        "                img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
        "                count+=1\n",
        "                max_length.append(count)\n",
        "                newPath2 = os.path.join(allDir, str(count) + '.png')\n",
        "                cv2.imwrite(newPath2, img)\n",
        "                image_label.append([newPath2, label, count])\n",
        "            \n",
        "combine_images2(newfolders)\n",
        "print(\"done\")\n",
        "print(len(max_length))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n",
            "20895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk2degKSbfsC"
      },
      "source": [
        "labelsEveryFolder = []\n",
        "for i in range(5):\n",
        "    temp = []\n",
        "    labelsEveryFolder.append(temp)\n",
        "sample = []\n",
        "def create_random_folds(allimages):\n",
        "    randFolder = 0\n",
        "    randArray = []\n",
        "    for i in range(5):\n",
        "        randPath = os.path.join(origDir, 'Random' + str(i))\n",
        "        if os.path.isdir(randPath):\n",
        "            shutil.rmtree(randPath)\n",
        "        os.mkdir(randPath)\n",
        "        with open(os.path.join(randPath, \"label.csv\"), 'w+') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(['origpath', 'label', 'directory'])\n",
        "            pass\n",
        "        randArray.append(randPath)\n",
        "    random.shuffle(allimages)\n",
        "    max_num = math.ceil(0.2*len(max_length))\n",
        "    count = 0\n",
        "    for row in allimages:\n",
        "        imagePath = row[0]\n",
        "        if count < max_num:\n",
        "            temp = []\n",
        "            temp.append(imagePath)\n",
        "            temp.append(row[1])\n",
        "            path = os.path.join(origDir, 'Random' + str(randFolder))\n",
        "            shutil.copy(imagePath, path)\n",
        "            temp.append(path)\n",
        "            sample.append(temp)\n",
        "            labelsEveryFolder[randFolder].append(temp)\n",
        "            with open(os.path.join(path, \"label.csv\"), 'a') as csvfile:\n",
        "                csvwriter = csv.writer(csvfile, delimiter=',', lineterminator='\\r') \n",
        "                csvwriter.writerow(temp)\n",
        "            temp = []\n",
        "            count+=1\n",
        "        else:\n",
        "            randFolder+=1\n",
        "            count = 0\n",
        "    return randArray\n",
        "datasetfolders = create_random_folds(image_label)\n",
        "# print(datasetfolders)\n",
        "# print(labelsEveryFolder)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaaUr85IaSPr"
      },
      "source": [
        "class DataSet:\n",
        "\n",
        "    def __init__(self, root):\n",
        "    \n",
        "        self.ROOT = root\n",
        "        self.images, self.mean, self.std = self.read_images(self.ROOT)\n",
        "        self.labels = self.read_labels(self.ROOT)\n",
        "        # Data normalization\n",
        "        self.transform = transforms.Compose([\n",
        "#         transforms.Grayscale(num_output_channels=1), # Convert image to grayscale\n",
        "            transforms.ToTensor(), # Transform from [0,255] uint8 to [0,1] float\n",
        "            transforms.Normalize((self.mean,), (self.std,))# TODO: Normalize to zero mean and unit variance with appropriate parameters\n",
        "        ])\n",
        "        \n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "    # Return number of points in the dataset\n",
        "\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "    # Here we have to return the item requested by `idx`. The PyTorch DataLoader class will use this method to make an iterable for training/validation loop.\n",
        "\n",
        "        img = self.transform(self.images[idx])\n",
        "        img = np.concatenate((img,img,img), 0)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return img, label\n",
        "    \n",
        "    def read_images(self, path:str) -> list:\n",
        "        imgs = []\n",
        "        mean = 0\n",
        "        std = 0\n",
        "        count = 0\n",
        "        for filename in sorted([f for f in os.listdir(path) if f.endswith(\".png\")], key=lambda f : int(f[:-4])): \n",
        "            img = cv2.imread(os.path.join(path, filename), cv2.IMREAD_UNCHANGED)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            imgs.append(img)\n",
        "            mean += np.mean(img)\n",
        "            std += np.var(img)\n",
        "            count += 1\n",
        "        mean /= count*255\n",
        "        std = math.sqrt(std / (count)) / 255\n",
        "        print(path)\n",
        "        return imgs, mean, std\n",
        "    \n",
        "    def read_labels(self, path:str) -> list:\n",
        "        labels = pd.read_csv(os.path.join(path, 'label.csv'))\n",
        "        print(labels)\n",
        "        return labels.label\n",
        "\n",
        "# Load the dataset and train and test splits\n",
        "print(\"Loading datasets...\")\n",
        "print(\"Done!\")\n",
        "print(\"Creating datasets\")\n",
        "split = []\n",
        "everything = []\n",
        "for i in range(5):\n",
        "    curr = os.path.join(origDir, 'Random' + str(i))\n",
        "    everything.append(DataSet(curr))\n",
        "    split.append(DataSet(curr))\n",
        "print(\"Done done\")\n",
        "print(everything)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaWYyJS1aSNd"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: [Transfer learning with pre-trained ResNet-50] 1) Define how many first layers of convolutoinal neural network (CNN) feature extractor in ResNet-50 to be \"frozen\" and 2) design your own fully-connected network (FCN) classifier.\n",
        "        # 1) You will only refine last several layers of CNN feature extractor in ResNet-50 that mainly relate to high-level vision task. Determine how many first layers of ResNet-50 should be frozen to achieve best performances. Commented codes below will help you understand the architecture, i.e., \"children\", of ResNet-50.\n",
        "        # 2) Design your own FCN classifier. Here I provide a sample of two-layer FCN.\n",
        "        # Refer to PyTorch documentations of torch.nn to pick your layers. (https://pytorch.org/docs/stable/nn.html)\n",
        "        # Some common Choices are: Linear, ReLU, Dropout, MaxPool2d, AvgPool2d\n",
        "        # If you have many layers, consider using nn.Sequential() to simplify your code\n",
        "        \n",
        "        # Load pretrained ResNet-50\n",
        "        self.model_resnet = models.resnet50(pretrained=True)\n",
        "        \n",
        "        # The code below can show children of ResNet-50\n",
        "        # child_counter = 0\n",
        "        # for child in self.model_resnet.children():\n",
        "        #    print(\" child\", child_counter, \"is -\")\n",
        "        #    print(child)\n",
        "        #    child_counter += 1\n",
        "        \n",
        "            \n",
        "        # TODO: Determine how many first layers of ResNet-50 to freeze\n",
        "        #first iteration freeze all children\n",
        "        child_counter = 0\n",
        "        for child in self.model_resnet.children():\n",
        "            if child_counter < params['frozenChildren']:\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = False\n",
        "            elif child_counter == params['frozenChildren']:\n",
        "                children_of_child_counter = 0\n",
        "                for children_of_child in child.children():\n",
        "                    # if children_of_child_counter < ???:\n",
        "                    #     for param in children_of_child.parameters():\n",
        "                    #         para.requires_grad = False\n",
        "                    # else:\n",
        "                    children_of_child_counter += 1\n",
        "            else:\n",
        "                print(\"child \",child_counter,\" was not frozen\")\n",
        "            child_counter += 1\n",
        "            \n",
        "        #first iteration start with classifier relu\n",
        "        \n",
        "        # Set ResNet-50's FCN as an identity mapping\n",
        "        num_fc_in = self.model_resnet.fc.in_features\n",
        "        self.model_resnet.fc = nn.Identity()\n",
        "    \n",
        "\n",
        "        self.fc1 = nn.Linear(num_fc_in, 256, bias = False)\n",
        "        self.b1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128, bias = False)\n",
        "        self.b2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 64, bias = False)\n",
        "        self.b3 = nn.BatchNorm1d(64)\n",
        "        self.fc4 = nn.Linear(64, 3, bias = False)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        # TODO: Design your own network, implement forward pass here\n",
        "        \n",
        "        relu = nn.ReLU() # No need to define self.relu because it contains no parameters\n",
        "        gelu = nn.GELU()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            features = self.model_resnet(x)\n",
        "        \n",
        "        \n",
        "        #maybe add batch normalizations\n",
        "        x = self.fc1(features) # Activation are flattened before being passed to the fully connected layers\n",
        "        x = self.b1(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.b2(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.b3(x)\n",
        "        x = relu(x)\n",
        "        x = self.fc4(x)\n",
        "        \n",
        "        # The loss layer will be applied outside Network class\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNtEYPhuaSLX"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Configure device\n",
        "model= Network().to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Specify the loss layer (note: CrossEntropyLoss already includes LogSoftMax())\n",
        "# # TODO: Modify the line below, experiment with different optimizers and parameters (such as learning rate)\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=params['learningRate'])\n",
        "num_epochs = params['epochs'] # TODO: Choose an appropriate number of training epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbhF1Q5BaSJF"
      },
      "source": [
        "avgTraining = []\n",
        "avgTesting = []\n",
        "\n",
        "plotTraining = []\n",
        "plotTesting = []\n",
        "\n",
        "def train(model, loader, test, num_epoch, optimizer): # Train the model\n",
        "    print(\"Start training...\")\n",
        "    model.train() # Set the model to training mode\n",
        "    trainingLoss = []\n",
        "    testingLoss = []\n",
        "    for i in range(num_epoch):\n",
        "        running_loss = []\n",
        "        testing_loss = []\n",
        "        for batch, label in tqdm(loader, position=0, leave=True, disable=True):\n",
        "            batch = batch.to(device)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad() # Clear gradients from the previous iteration\n",
        "            pred = model(batch) # This will call Network.forward() that you implement\n",
        "            loss = criterion(pred, label) # Calculate the loss\n",
        "            running_loss.append(loss.item())\n",
        "            loss.backward() # Backprop gradients to all tensors in the network\n",
        "            optimizer.step() # Update trainable weights\n",
        "        print(\"Epoch {} training loss:{}\".format(i+1,np.mean(running_loss))) # Print the average loss for this epoch\n",
        "        with torch.no_grad():\n",
        "            for batch, label in tqdm(test, disable=True):\n",
        "                batch = batch.to(device)\n",
        "                label = label.to(device)\n",
        "                pred = model(batch)\n",
        "                loss = criterion(pred, label)\n",
        "                testing_loss.append(loss.item()) \n",
        "        print(\"Epoch {} testing loss:{}\".format(i+1,np.mean(testing_loss))) \n",
        "        trainingLoss.append(np.mean(running_loss))\n",
        "        testingLoss.append(np.mean(testing_loss))  \n",
        "    \n",
        "    print(\"Done!\")\n",
        "    avgTraining.append(trainingLoss)\n",
        "    avgTesting.append(testingLoss)\n",
        "    plt.plot(trainingLoss, color='blue', label=\"train\")\n",
        "    plt.plot(testingLoss, color='red', label=\"test\")\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate(model, loader): # Evaluate accuracy on validation / test set\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    with torch.no_grad(): # Do not calculate grident to speed up computation\n",
        "        for batch, label in tqdm(loader):\n",
        "            batch = batch.to(device)\n",
        "            #set batch size\n",
        "            label = label.to(device)\n",
        "            pred = model(batch)\n",
        "            correct += (torch.argmax(pred,dim=1)==label).sum().item()\n",
        "    acc = correct/len(loader.dataset)\n",
        "    print(\"Evaluation accuracy: {}\".format(acc))\n",
        "\n",
        "    return acc\n",
        "    \n",
        "# train(model, trainloader, num_epochs)\n",
        "# print(\"Evaluate on test set\")\n",
        "# evaluate(model, testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K6ZZrS2aSG5"
      },
      "source": [
        "x = [0,1,2,3,4]\n",
        "testIndex = []\n",
        "trainIndex = []\n",
        "\n",
        "\n",
        "print(\"Begin training...\")\n",
        "\n",
        "for i in range(5):\n",
        "    model = Network().to(device)\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=params['learningRate']) # Specify optimizer and assign trainable parameters to it, weight_decay is L2 regularization strength (default: lr=1e-2, weight_decay=1e-4)\n",
        "    testIndex.append(x[i])\n",
        "    x.remove(i)\n",
        "    for j in range(4):\n",
        "        trainIndex.insert(j, x[j])\n",
        "    x.insert(i, i)\n",
        "    batchTrain = DataLoader(ConcatDataset([everything[trainIndex[0]], everything[trainIndex[1]], \n",
        "                                           everything[trainIndex[2]], everything[trainIndex[3]]]),\n",
        "                         batch_size = params['batchSize'],\n",
        "                         shuffle = True, drop_last = True)\n",
        "    batchTest = DataLoader(everything[testIndex[0]], batch_size = params['batchSize'], \n",
        "                        shuffle = True, drop_last = True)\n",
        "    testIndex = []\n",
        "    trainIndex = []\n",
        "    training = train(model, batchTrain, batchTest, num_epochs, optimizer)\n",
        "    averageAccuracy1.append(evaluate(model, batchTest))\n",
        "    averageAccuracy2.append(evaluate(model, batchTrain))\n",
        "    print(f'Testing accuracy for batch {i}: {evaluate(model, batchTest)}')\n",
        "    del model\n",
        "\n",
        "print(avgTraining)\n",
        "print(avgTesting)\n",
        "for x in range(params['epochs']):\n",
        "  tempArray1 = []\n",
        "  tempArray2 = []\n",
        "  for y in range(5):\n",
        "    tempArray1.append(avgTraining[y][x])\n",
        "    tempArray2.append(avgTesting[y][x])\n",
        "  plotTraining.append(np.mean(tempArray1))\n",
        "  plotTesting.append(np.mean(tempArray2))\n",
        "  print(tempArray1)\n",
        "  print(tempArray2)\n",
        "\n",
        "plt.plot(plotTraining, color='blue', label=\"training avg\")\n",
        "plt.plot(plotTesting, color='red', label=\"testing avg\")\n",
        "plt.ylabel('loss avg')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f'Average Training accuracy: {np.mean(averageAccuracy2)}')\n",
        "print(f'Average Testing accuracy: {np.mean(averageAccuracy1)}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyZ8_ERSaSEU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8c62at0aR-q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
